{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor implementation using python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to ensemble techniques, nothing beats random forests. They are as stable and robust as any other new technique in the world of machine learning. They are built on one concept. \n",
    "\n",
    "If I can get together enough experts' opinions and take an average call, they my final decision will be better as compared to taking a call based on the same no of less expert people's opinion. \n",
    "\n",
    "This is achieved by randomness and our experts are basically decision trees. ech tree builds a biased model on a set of data that can predict something very nicely but not generalised enough. So we take a group(ensemble) of decision trees and make this a forest and take the average opinion. \n",
    "\n",
    "This is precisely what we have built here. \n",
    "\n",
    "Source : http://course18.fast.ai/lessonsml1/lesson7.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic import *\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic structure and pseudo code.\n",
    "\n",
    "Random forest:\n",
    "\n",
    "    make n_tree trees\n",
    "    fit each tree with randomly selected samples\n",
    "    \n",
    "    for predictions: \n",
    "        Take mean of all tree predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestR():\n",
    "    def __init__(self, x, y, n_trees=10, min_leaf_samples=3, n_samples=50):\n",
    "        np.random.seed(42)\n",
    "        self.x, self.y, self.min_leaf_samples, self.n_samples = x, y, min_leaf_samples, n_samples\n",
    "        self.trees = []\n",
    "        for i in range(n_trees):\n",
    "            rnd_ids = np.random.permutation(len(self.y))[:self.n_samples]\n",
    "            self.trees.append(DecisionTree(self.x.iloc[rnd_ids], self.y[rnd_ids], np.array(range(len(rnd_ids))), self.min_leaf_samples))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.mean([t.predict(x) for t in self.trees], axis=0)\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        return r2_score(y, self.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree:\n",
    "    \n",
    "    Go through all columns and all values to find the minimum score\n",
    "    make a split at this value\n",
    "    create a left tree\n",
    "    create a right tree\n",
    "    The value at each tree head/node will be the mean of samples.\n",
    "    \n",
    "    for predictions:\n",
    "        Go through the tree and return the value at the landing leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_agg(cnt, sum1, sum2):\n",
    "    return math.sqrt((sum2/cnt) - (sum1/cnt)**2)\n",
    "\n",
    "class DecisionTree():\n",
    "    def __init__(self, x, y, tree_ids, min_leaf_samples=3):\n",
    "        self.x, self.y, self.tree_ids, self.min_leaf_samples = x, y, tree_ids, min_leaf_samples\n",
    "        self.n, self.ncols = len(self.tree_ids), self.x.shape[1]\n",
    "        self.mse = float('inf')\n",
    "        self.score = np.mean(y[self.tree_ids])\n",
    "        self.find_splits()\n",
    "    \n",
    "    def find_splits(self):\n",
    "        for i in range(self.ncols):\n",
    "            self.find_better_split(i)\n",
    "        if self.is_leaf: \n",
    "            return\n",
    "        x = self.x.values[self.tree_ids, self.split_col]\n",
    "        lhs_ids = np.nonzero(x <= self.split_val)[0]\n",
    "        rhs_ids = np.nonzero(x > self.split_val)[0]\n",
    "        self.lhs = DecisionTree(self.x, self.y, self.tree_ids[lhs_ids])\n",
    "        self.rhs = DecisionTree(self.x, self.y, self.tree_ids[rhs_ids])\n",
    "        \n",
    "    def find_better_split(self, column):\n",
    "        x = self.x.values[self.tree_ids, column]\n",
    "        y = self.y[self.tree_ids]\n",
    "        sorted_indxs = np.argsort(x)\n",
    "        x_sort, y_sort = x[sorted_indxs], y[sorted_indxs]\n",
    "        \n",
    "        rhs_cnt = self.n\n",
    "        lhs_cnt = 0\n",
    "        \n",
    "        rhs_sum = y_sort.sum()\n",
    "        lhs_sum = 0\n",
    "        \n",
    "        rhs_sum2 = (y_sort**2).sum()\n",
    "        lhs_sum2 = 0\n",
    "        \n",
    "        for i in range(0, self.n-self.min_leaf_samples-1):\n",
    "            lhs_cnt += 1\n",
    "            rhs_cnt -= 1\n",
    "            lhs_sum += y_sort[i]\n",
    "            rhs_sum -= y_sort[i]\n",
    "            lhs_sum2 += y_sort[i]**2\n",
    "            rhs_sum2 -= y_sort[i]**2\n",
    "            if i < self.min_leaf_samples or x_sort[i]==x_sort[i+1]:\n",
    "                continue\n",
    "            lhs_std = std_agg(lhs_cnt, lhs_sum, lhs_sum2)\n",
    "            rhs_std = std_agg(rhs_cnt, rhs_sum, rhs_sum2)\n",
    "            curr_score = lhs_cnt*lhs_std + rhs_cnt*rhs_std\n",
    "            if curr_score < self.mse:\n",
    "                self.mse, self.split_val, self.split_col = curr_score, x_sort[i], column\n",
    "    @property       \n",
    "    def is_leaf(self):\n",
    "        return self.mse == float('inf')\n",
    "    \n",
    "    def predict(self,x):\n",
    "        return [self.predict_row(row) for index,row in x.iterrows()]\n",
    "    \n",
    "    def predict_row(self, row):\n",
    "        if self.is_leaf:\n",
    "            return self.score\n",
    "        if row[self.split_col] < self.split_val:\n",
    "            return self.lhs.predict_row(row)\n",
    "        else: return self.rhs.predict_row(row)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        output = f'samples: {self.n}, value: {self.score}'\n",
    "        if not self.is_leaf:\n",
    "            output += f' column: {self.x.columns[self.split_col]}, value: {self.split_val}'\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the boston dataset for regression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_boston()\n",
    "X_train = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "y_train = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['DIS', 'INDUS', 'LSTAT', 'NOX', 'RAD', 'TAX', 'ZN']\n",
    "X_train = X_train.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestR(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6348998925187646"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=10,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=10, n_jobs=None, oob_score=False,\n",
       "                      random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_sklearn = RandomForestRegressor(n_estimators=10, min_samples_leaf=10)\n",
    "rf_sklearn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.784036670729276"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_sklearn.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The r2 score for our model and the sklearn model is not too close but still good enough for a scratch implementation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
